{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSSC 2 \n",
    "# Exercise 1: MPI-Parallelization of a Jacobi Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group No. 12\n",
    "\n",
    "Aman Bhardwaj (12333472)\n",
    "\n",
    "Florian Frech (12308544)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describe the advantages / disadvantages of a two-dimensional decomposition \n",
    "(compared to a one dimensional decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of a 2D decomposition:\n",
    "\n",
    "- It is easier to handle complex geometries which are irregular or intricate in shape as it decomposes the domain into smaller subdomains that can be handled and distributed evenly across individual processes.\n",
    "\n",
    "- It provides clear representation of discontinuous properties and interfaces, thus making it easier to apply  arious numerical methods and boundary conditions leading to more accurate simulations.\n",
    "\n",
    "- In a 2-D decomposition, each region can independently control and generate its own mesh, that provides more efficient mesh generation.\n",
    "\n",
    "- The neighbouring grid points close to each other are assigned the same MPI process which helps in better cache utilization and more data locality.\n",
    "\n",
    "- Conservative methods of physical quantities can be employed because they can be applied to individual regions independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of a 2D decomposition:\n",
    "\n",
    "- Due to less connectivity of information between the neighbouring processes, it can become more challenging to implement and handle communication between neighbouring processes, which in turn lead to more complexity and potential errors.\n",
    "\n",
    "- In some cases when the grid size is large in 2-D decomposition, it can result in increased communication volume as the information is to be exchanged in both dimensions.\n",
    "\n",
    "- Storing stencil coefficients for each control volume within each MPI process can lead to increased memory usage, when the grid size is large and reduce scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Discuss if the decomposition of the domain changes the order of computations performed during a single jacobi iteration\n",
    "(i.e., if you expect a numerically identical result after each iteration, or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Domain decomposition, the order of computations within a single Jacobi iteration does not change and remains consistent as each MPI process independently updates the values of grid points within the assigned sub-domain which are based on the values of previous iteration from the neighbouring grid\n",
    "points.\n",
    "\n",
    "Also, because the computations proceed sequentially within each segment and subdomain, each grid point is  updated one after the other in the decompositions leading to numerically identical result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. An extension to the ghost layer would be to use a wider layer (of more than one ghost cell). This allows to perform multiple independent iterations before a communication of the ghost layer has to take happen.\n",
    "Comment in which situation (w.r.t. the available bandwidth or latency between MPI-processes) multiple independent iterations are potentially advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a wider layer to perform multiple independent iterations before setting up a communication of the ghost layers can be advantageous in many ways in the cases when the bandwidth between MPI processes is high, domain is large and  the latency is low, so this decreases the time spent on communication compared to that of computation.\n",
    "\n",
    "It will result in better utilization of the resources and improved performance of the Jacobi solver. Also, all processes can proceed in a similar pace and reduce communication delays ensuring more balanced progress across the entire domain.\n",
    "\n",
    "However, also the amount of required memory increases proportionally which cannot be ignored in all setups.\n",
    "\n",
    "In general, using multiple ghost layers and performing multiple independet iterations is benefitial, if the latency of the process communication is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How big is the sum of all L2 caches for 2 nodes of the IUE-cluster\n",
    "\n",
    "The IUE-cluster is based on a Dell Power Edge R740 platform and consits of 10 regular compute nodes, 2 fat compute nodes and 1 login/storage node.\n",
    "\n",
    "The regular compute nodes uses two Intel Xeon Gold 6248 processors at 2.5 GHz with 20 cores and 40 available threads.\n",
    "\n",
    "The cache size of each node is $L2_{single} = 27.5 \\space MB$. (https://ark.intel.com/content/www/de/de/ark/products/192446/intel-xeon-gold-6248-processor-27-5m-cache-2-50-ghz.html)\n",
    "\n",
    "However, this is the sum of the total cache. For this kind of processor the Intel Xeon Gold 6000 series uses 1 MB L2 cache per processor.\n",
    "(https://www.techpowerup.com/cpu-specs/xeon-gold-6342.c2440).\n",
    "\n",
    "Thus the sum of all L2 caches for two nodes is:\n",
    "\n",
    "$$\n",
    "L2_{total} = 2 \\cdot \\frac{CPUs}{Node} \\cdot * L_{single} \\frac{MB}{CPU} = 4 \\cdot 20 \\space MB = 80 \\space MB\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
